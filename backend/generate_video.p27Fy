import os
import json
import math
import glob
import shutil
import requests
import base64
from typing import List, Dict, Any, Tuple
from PIL import Image
from backend.meme_injector import inject_random_memes
from backend.render_bubble import add_still_to_concat, handle_meme_image
import subprocess
import shlex


# --------------------
# Paths & defaults
# --------------------
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
FRAMES_DIR = os.path.join(BASE_DIR, "frames")
TIMELINE_FILE = os.path.join(FRAMES_DIR, "timeline.json")
BG_TIMELINE_FILE = os.path.join(FRAMES_DIR, "bg_timeline.json")
STATIC_AUDIO = os.path.join(BASE_DIR, "static", "audio")
OUTPUT_VIDEO = os.path.join(BASE_DIR, "output.mp4")

TMP_DIR = os.path.join(BASE_DIR, "tmp_ffmpeg")
os.makedirs(TMP_DIR, exist_ok=True)

DEFAULT_BG = os.path.join(STATIC_AUDIO, "default_bg.mp3")
DEFAULT_SEND = os.path.join(STATIC_AUDIO, "send.mp3")
DEFAULT_RECV = os.path.join(STATIC_AUDIO, "recv.mp3")

FPS = 25  # Target frame rate

# --------------------
# Helpers
# --------------------

def build_typing_audio_sessions(timeline, typing_sound_master_path, tmp_dir):
    """
    TEMPORARY FIXED VERSION: Handles missing action/sound fields
    """
    os.makedirs(tmp_dir, exist_ok=True)
    sessions = {}
    current_session = None

    print("ðŸŽ¹ Analyzing timeline for typing sessions...")
    
    for idx, entry in enumerate(timeline):
        session_id = entry.get("typing_session_id")
        action = entry.get("typing_session_action", "")
        duration = float(entry.get("duration", 0.0))
        sound_enabled = entry.get("sound", False)
        
        # TEMPORARY FIX: Handle missing fields
        if session_id:
            # If action is missing, infer it
            if not action:
                if current_session is None or current_session["id"] != session_id:
                    action = "start"
                else:
                    action = "continue"
            
            # If sound is missing, default to True (play sound)
            if sound_enabled is None:
                sound_enabled = True
                
            print(f"ðŸŽ¹ Frame {idx}: session={session_id}, action={action}, sound={sound_enabled}, duration={duration:.3f}s")

        if action == "start":
            current_session = {
                "id": session_id,
                "start_idx": idx,
                "start_time": sum(float(t.get("duration", 0)) for t in timeline[:idx]),
                "total_duration": duration if sound_enabled else 0,
                "sound_frames": [idx] if sound_enabled else [],
                "last_action": action
            }
            print(f"ðŸŽ¹ START session {session_id} at frame {idx}")
            
        elif action in ["continue", "pause"] and current_session and current_session["id"] == session_id:
            if sound_enabled:
                current_session["total_duration"] += duration
                current_session["sound_frames"].append(idx)
            current_session["last_action"] = action
            print(f"ðŸŽ¹ {action.upper()} session {session_id} - sound: {sound_enabled}")
            
        elif action == "stop" and current_session and current_session["id"] == session_id:
            if sound_enabled:
                current_session["total_duration"] += duration
                current_session["sound_frames"].append(idx)
            sessions[session_id] = current_session
            print(f"ðŸŽ¹ STOP session {session_id} - total sound duration: {current_session['total_duration']:.3f}s")
            current_session = None

    # Finalize if session is still open
    if current_session:
        print(f"ðŸŽ¹ WARNING: Unfinished session {current_session['id']} - forcing stop")
        sessions[current_session["id"]] = current_session

    # Create audio files for each session (rest of the function remains the same)
    trimmed_map = {}
    session_count = 0
    
    for session_id, session_info in sessions.items():
        duration = session_info["total_duration"]
        sound_frame_count = len(session_info.get("sound_frames", []))
        
        if duration <= 0 or sound_frame_count == 0:
            print(f"ðŸŽ¹ Skipping session {session_id}: no sound-enabled frames (duration={duration:.3f}s, frames={sound_frame_count})")
            continue
            
        # Create trimmed audio file for this session
        out_file = os.path.join(tmp_dir, f"typing_{session_count}_{session_id}.aac")
        
        try:
            cmd = [
                'ffmpeg', '-y', '-hide_banner', '-loglevel', 'error',
                '-i', typing_sound_master_path,
                '-t', str(duration),
                '-c:a', 'aac', '-b:a', '192k',
                out_file
            ]
            subprocess.run(cmd, check=True, capture_output=True)
            
            trimmed_map[session_id] = {
                "file": out_file,
                "duration": duration,
                "start_time": session_info["start_time"],
                "start_idx": session_info["start_idx"],
                "sound_frames": session_info.get("sound_frames", [])
            }
            session_count += 1
            print(f"ðŸŽ¹ Created typing audio for session {session_id}: {duration:.3f}s from {sound_frame_count} frames at {session_info['start_time']:.2f}s")
            
        except subprocess.CalledProcessError as e:
            print(f"âŒ Failed to create typing audio for session {session_id}: {e}")
        except Exception as e:
            print(f"âŒ Error creating typing audio for session {session_id}: {e}")

    print(f"ðŸŽ¹ Successfully created {len(trimmed_map)} typing audio sessions")
    return trimmed_map

def timeline_time_at_index(timeline, idx):
    """Calculate cumulative time up to a specific index in the timeline"""
    return sum(float(t.get("duration", 0)) for t in timeline[:idx])


def _run(cmd: str):
    print("RUN:", cmd)
    subprocess.check_call(cmd, shell=True)


def _safe(path: str) -> str:
    return path.replace("\\", "/")


def ensure_local(path_or_url: str) -> str:
    """
    If given a URL, download to TMP_DIR and return local path.
    If given an absolute local path, return it unchanged.
    If given a relative local path, return BASE_DIR + relative path.
    """
    if not path_or_url:
        return ""

    if isinstance(path_or_url, str) and path_or_url.startswith("http"):
        local_path = os.path.join(TMP_DIR, os.path.basename(path_or_url.split("?")[0]))
        if not os.path.exists(local_path):
            r = requests.get(path_or_url, stream=True, timeout=20)
            r.raise_for_status()
            with open(local_path, "wb") as f:
                for chunk in r.iter_content(1024 * 64):
                    f.write(chunk)
        return local_path

    if os.path.isabs(path_or_url):
        return path_or_url
    return os.path.join(BASE_DIR, path_or_url)


def _decode_meme_b64(item: Dict[str, Any], index: int) -> str:
    """
    If item contains meme_b64, decode it into TMP_DIR and return file path.
    Otherwise return None.
    """
    if not item.get("meme_b64"):
        return None

    # pick extension from hint or default
    ext = item.get("ext", ".png")
    out_path = os.path.join(TMP_DIR, f"meme_{index}{ext}")

    try:
        with open(out_path, "wb") as f:
            f.write(base64.b64decode(item["meme_b64"]))
        item["file"] = out_path
        return out_path
    except Exception as e:
        print(f"âš ï¸ Failed to decode meme_b64 for item {index}: {e}")
        return None


def _is_valid_image(path: str) -> bool:
    try:
        with Image.open(path) as im:
            im.verify()
        return True
    except Exception:
        return False


def create_concat_file_from_frames_only(frames_dir: str, concat_path: str, fps: int = FPS) -> Tuple[float, List[str]]:
    frames = sorted(glob.glob(os.path.join(frames_dir, "*.png")))
    frames = [f for f in frames if _is_valid_image(f)]
    if not frames:
        raise RuntimeError(f"No valid frames found in {frames_dir}")
    frame_duration = 1.0 / fps
    total_duration = 0.0
    lines = ["ffconcat version 1.0"]
    for frame in frames:
        add_still_to_concat(lines, _safe(frame), frame_duration)
        total_duration += frame_duration
    lines.append(f"file '{_safe(frames[-1])}'")
    with open(concat_path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines) + "\n")
    print(f"âœ… concat.txt (fallback) with {len(frames)} frames @ {fps}fps")
    return total_duration, frames


def _prepare_meme_clip(src_path: str, out_path: str, hold_seconds: float, video_w: int, video_h: int):
    # Ensure proper scaling + enforce even dimensions
    vf = (
        f"scale={video_w}:{video_h}:force_original_aspect_ratio=decrease,"
        f"scale=trunc(iw/2)*2:trunc(ih/2)*2,fps={FPS}"
    )
    cmd = (
        f'ffmpeg -y -i "{src_path}" -t {hold_seconds:.3f} -an '
        f'-vf "{vf}" -pix_fmt yuv420p -r {FPS} '
        f'-c:v libx264 -preset veryfast -crf 18 "{out_path}"'
    )
    _run(cmd)


def _process_meme_item(item, index, video_w, video_h, tmp_dir):
    # Check if file exists and is valid
    if "file" not in item or not item["file"]:
        print(f"âš ï¸ Meme {index}: No file path specified, skipping.")
        return None
        
    meme_src = ensure_local(item["file"])
    
    # Check if file actually exists
    if not os.path.exists(meme_src):
        print(f"âš ï¸ Meme {index}: File not found: {meme_src}, skipping.")
        return None
        
    hold = float(item.get("duration", 2.5))
    ext = os.path.splitext(meme_src)[1].lower()

    if ext in (".png", ".jpg", ".jpeg", ".webp"):
        try:
            out_frame_path, seconds = handle_meme_image(meme_src, os.path.join(TMP_DIR, f"meme_{index}.png"), hold)
            # Ensure out_frame_path is a string, not a list
            if isinstance(out_frame_path, list):
                if out_frame_path:
                    out_frame_path = out_frame_path[0]  # Use first frame
                    print(f"âš ï¸ Meme {index}: handle_meme_image returned list, using first frame: {out_frame_path}")
                else:
                    print(f"âš ï¸ Meme {index}: handle_meme_image returned empty list, skipping.")
                    return None
            if not os.path.exists(out_frame_path) or not _is_valid_image(out_frame_path):
                print(f"âš ï¸ Meme {index}: Invalid image output {out_frame_path}, skipping.")
                return None
            return {"type": "image", "path": out_frame_path, "duration": seconds}
        except Exception as e:
            print(f"âš ï¸ Meme {index} image processing failed: {e}")
            return None

    elif ext in (".gif", ".mp4", ".mov", ".mkv", ".webm"):
        meme_clip = os.path.join(tmp_dir, f"meme_{index}.mp4")
        try:
            _prepare_meme_clip(meme_src, meme_clip, hold, video_w, video_h)
            return {"type": "video", "path": meme_clip, "duration": hold}
        except Exception as e:
            print(f"âš ï¸ Meme {index} conversion failed: {e}. Falling back to thumbnail.")
            try:
                thumb, seconds = handle_meme_image(meme_src, os.path.join(TMP_DIR, f"meme_{index}_thumb.png"), min(hold, 2.0))
                if isinstance(thumb, list):
                    if thumb:
                        thumb = thumb[0]
                        print(f"âš ï¸ Meme {index}: Thumbnail returned list, using first: {thumb}")
                    else:
                        print(f"âš ï¸ Meme {index}: Thumbnail returned empty list, skipping.")
                        return None
                if not os.path.exists(thumb) or not _is_valid_image(thumb):
                    print(f"âš ï¸ Meme {index}: Invalid thumbnail {thumb}, skipping.")
                    return None
                return {"type": "image", "path": thumb, "duration": seconds}
            except Exception as e2:
                print(f"âš ï¸ Meme {index} thumbnail also failed: {e2}")
                return None
    else:
        print(f"âš ï¸ Meme {index}: unsupported extension {ext}, skipping.")
        return None


def _infer_canvas_size_from_first_frame(timeline: List[Dict[str, Any]], default_w=1904, default_h=934) -> Tuple[int, int]:
    for item in timeline:
        if not item.get("is_meme"):
            f = item.get("frame")
            if f:
                frame_path = os.path.join(BASE_DIR, f) if not os.path.isabs(f) else f
                if os.path.exists(frame_path) and _is_valid_image(frame_path):
                    try:
                        with Image.open(frame_path) as im:
                            return im.width, im.height
                    except Exception:
                        pass
    return default_w, default_h


def debug_timeline_loading():
    """Debug timeline loading and frame paths"""
    print("ðŸ” ===== TIMELINE DEBUG =====")
    
    if os.path.exists(TIMELINE_FILE):
        with open(TIMELINE_FILE, "r", encoding="utf-8") as f:
            timeline = json.load(f)
        
        print(f"ðŸ” Timeline entries: {len(timeline)}")
        total_duration = 0
        for i, item in enumerate(timeline):
            duration = item.get("duration", 0)
            total_duration += duration
            frame_path = item.get("frame", "")
            has_frame = os.path.exists(frame_path) if frame_path else False
            print(f"ðŸ” Entry {i}: duration={duration}s, frame='{frame_path}', exists={has_frame}")
            if frame_path and not has_frame:
                # Try to find the frame
                abs_path = os.path.join(BASE_DIR, frame_path) if not os.path.isabs(frame_path) else frame_path
                print(f"ðŸ”   Absolute path: {abs_path}, exists: {os.path.exists(abs_path)}")
        
        print(f"ðŸ” Total expected duration: {total_duration}s")
        return timeline, total_duration
    else:
        print("ðŸ” No timeline file found!")
        return [], 0
    

def debug_typing_timeline_entries(timeline):
    """Debug function to check typing entries in timeline"""
    print("ðŸ” ===== TYPING TIMELINE ENTRIES DEBUG =====")
    
    typing_entries = []
    for i, entry in enumerate(timeline):
        if entry.get("typing_session_id"):
            typing_entries.append((i, entry))
    
    print(f"ðŸ” Found {len(typing_entries)} typing entries in timeline")
    
    if not typing_entries:
        print("âŒ No typing entries found in timeline!")
        return
    
    for idx, entry in typing_entries:
        print(f"ðŸ” Frame {idx}:")
        print(f"   session_id: {entry.get('typing_session_id')}")
        print(f"   action: {entry.get('typing_session_action')}")
        print(f"   sound: {entry.get('sound')}")
        print(f"   duration: {entry.get('duration')}")
        print(f"   typing_bar: {entry.get('typing_bar')}")
        print(f"   typing: {entry.get('typing')}")
        print("   ---")


# --------------------
# Main builder
# --------------------

def build_video_from_timeline(bg_audio=None, send_audio=None, recv_audio=None, typing_audio=None, typing_bar_audio=None, use_segments=False, bg_segments: List[Dict[str, Any]] = None) -> str:
    print("ðŸŽ¬ ===== build_video_from_timeline CALLED =====")
    print(f"ðŸŽ¬ Parameters received:")
    print(f"ðŸŽ¬   bg_audio: {bg_audio}")
    print(f"ðŸŽ¬   send_audio: {send_audio}") 
    print(f"ðŸŽ¬   recv_audio: {recv_audio}")
    print(f"ðŸŽ¬   typing_audio: {typing_audio}")
    print(f"ðŸŽ¬   typing_bar_audio: {typing_bar_audio}")
    print(f"ðŸŽ¬   use_segments: {use_segments}")
    print(f"ðŸŽ¬   bg_segments param: {bg_segments}")

    # Initialize variables
    delayed_files: List[str] = []  # Sound effects
    delayed_bg_files: List[str] = []  # Background music ONLY

    # Debug timeline loading
    print("ðŸ” Debugging timeline and frames...")
    debug_timeline, expected_duration = debug_timeline_loading()

    total_duration = 0.0
    print("ðŸŽ¬ ===== build_video_from_timeline STARTED =====")
    
    # Check frames directory
    frames_in_dir = glob.glob(os.path.join(FRAMES_DIR, "*.png"))
    print(f"ðŸ” Frames in {FRAMES_DIR}: {len(frames_in_dir)}")

    # Clean up temp directory
    if os.path.exists(TMP_DIR):
        shutil.rmtree(TMP_DIR)
    os.makedirs(TMP_DIR, exist_ok=True)
    if os.path.exists(OUTPUT_VIDEO):
        os.remove(OUTPUT_VIDEO)

    concat_txt = os.path.join(TMP_DIR, "concat.txt")
    total_duration = 0.0
    timeline: List[Dict[str, Any]] = []
    all_segment_paths: List[str] = []

    # ------------------ LOAD TIMELINE ------------------
    if os.path.exists(TIMELINE_FILE):
        with open(TIMELINE_FILE, "r", encoding="utf-8") as f:
            timeline = json.load(f)

        print(f"ðŸŽ¬ Loaded {len(timeline)} timeline entries from file")

        # ADD THIS LINE - call the debug function
        debug_typing_timeline_entries(timeline)

        # Small delay between text & meme of same user
        for i, item in enumerate(timeline):
            if item.get("is_meme") and i > 0:
                prev = timeline[i - 1]
                if prev.get("text") and prev.get("username") == item.get("username") and not prev.get("is_meme"):
                    item["duration"] = item.get("duration", 2.0) + 0.5
                    print(f"â±ï¸ Added 0.5s delay between text & meme for {item['username']}")

        # Decode any base64 memes
        for i, item in enumerate(timeline):
            if item.get("meme_b64"):
                _decode_meme_b64(item, i)

        # Filter invalid entries
        valid_timeline = []
        for item in timeline:
            if item.get("typing"):
                valid_timeline.append(item)
                continue
            if item.get("is_meme") and ("file" not in item or not item["file"]):
                print(f"âš ï¸ Skipping meme missing file: {item}")
                continue
            if not item.get("is_meme") and ("frame" not in item or not item["frame"]):
                print(f"âš ï¸ Skipping frame missing path: {item}")
                continue
            valid_timeline.append(item)

        print(f"ðŸŽ¬ After filtering: {len(valid_timeline)} valid entries")

        # Inject random memes (if applicable)
        timeline = inject_random_memes(valid_timeline, chance=0.25, max_per_video=3)

        if timeline:
            video_w, video_h = _infer_canvas_size_from_first_frame(timeline)
            lines = ["ffconcat version 1.0"]
            meme_segments = []

            # ------------------ MAIN LOOP ------------------
            for i, item in enumerate(timeline):
                # --- Typing bubbles ---
                if item.get("typing"):
                    frame_path = os.path.join(BASE_DIR, item["frame"]) if not os.path.isabs(item["frame"]) else item["frame"]
                    if os.path.exists(frame_path) and _is_valid_image(frame_path):
                        seconds = float(item.get("duration", 1.5))
                        add_still_to_concat(lines, _safe(frame_path), seconds)
                        all_segment_paths.append(frame_path)
                        total_duration += seconds
                        print(f"âœ… Typing frame {i}: {frame_path} ({seconds}s)")
                    else:
                        print(f"âš ï¸ Typing frame {i}: missing or invalid {item.get('frame')}")
                    continue

                # --- Typing BAR (new) ---
                if item.get("typing_bar"):
                    frame_path = os.path.join(BASE_DIR, item["frame"]) if not os.path.isabs(item["frame"]) else item["frame"]
                    if os.path.exists(frame_path) and _is_valid_image(frame_path):
                        seconds = float(item.get("duration", 1.5))
                        add_still_to_concat(lines, _safe(frame_path), seconds)
                        all_segment_paths.append(frame_path)
                        total_duration += seconds
                        print(f"âœ… Typing BAR frame {i}: {frame_path} ({seconds}s) - upcoming_text: {item.get('upcoming_text', 'N/A')}")
                    else:
                        print(f"âš ï¸ Typing BAR frame {i}: missing or invalid {item.get('frame')}")
                        if "frame" in item:
                            frame_dir = os.path.dirname(frame_path)
                            if os.path.exists(frame_dir):
                                existing_files = os.listdir(frame_dir)
                                print(f"ðŸ” Files in frame directory: {existing_files[:10]}...")
                    continue

                # --- Regular chat frames ---
                if not item.get("is_meme"):
                    frame_path = os.path.join(BASE_DIR, item["frame"]) if not os.path.isabs(item["frame"]) else item["frame"]
                    if os.path.exists(frame_path) and _is_valid_image(frame_path):
                        seconds = float(item.get("duration", 1.5))
                        add_still_to_concat(lines, _safe(frame_path), seconds)
                        all_segment_paths.append(frame_path)
                        total_duration += seconds
                        print(f"âœ… Regular frame {i}: {frame_path} ({seconds}s)")
                        continue
                    else:
                        print(f"âš ï¸ Frame {i}: missing or invalid {item.get('frame')}")
                        continue

                # --- Meme chat frame priority ---
                if item.get("is_meme") and item.get("frame"):
                    frame_path = os.path.join(BASE_DIR, item["frame"]) if not os.path.isabs(item["frame"]) else item["frame"]
                    if os.path.exists(frame_path) and _is_valid_image(frame_path):
                        seconds = float(item.get("duration", 2.5))
                        add_still_to_concat(lines, _safe(frame_path), seconds)
                        all_segment_paths.append(frame_path)
                        total_duration += seconds
                        print(f"âœ… Used rendered chat frame for meme {i}: {frame_path} ({seconds}s)")
                        continue
                    else:
                        print(f"âš ï¸ Meme {i}: frame missing or invalid, fallback to meme asset")

                # --- Fallback: process raw meme asset ---
                if "file" not in item or not item["file"]:
                    print(f"âš ï¸ Meme {i}: No file specified, skipping.")
                    continue

                meme_result = _process_meme_item(item, i, video_w, video_h, TMP_DIR)
                if meme_result and os.path.exists(meme_result["path"]):
                    if meme_result["type"] == "image":
                        add_still_to_concat(lines, _safe(meme_result["path"]), meme_result["duration"])
                        all_segment_paths.append(meme_result["path"])
                        total_duration += meme_result["duration"]
                        print(f"âœ… Meme {i} processed as image: {meme_result['path']} ({meme_result['duration']}s)")
                    else:
                        lines.append(f"file '{_safe(meme_result['path'])}'")
                        all_segment_paths.append(meme_result["path"])
                        total_duration += meme_result["duration"]
                        print(f"âœ… Meme {i} processed as video: {meme_result['path']} ({meme_result['duration']}s)")
                else:
                    print(f"âš ï¸ Meme {i}: Processing failed, skipping.")

            # Add repeat of last frame
            if all_segment_paths:
                lines.append(f"file '{_safe(all_segment_paths[-1])}'")

            with open(concat_txt, "w", encoding="utf-8") as f:
                f.write("\n".join(lines) + "\n")

            print(f"ðŸŽ¬ Created concat file with {len(lines)} entries, total duration: {total_duration}s")

        else:
            print("ðŸŽ¬ No valid timeline entries, falling back to frames directory")
            total_duration, _ = create_concat_file_from_frames_only(FRAMES_DIR, concat_txt)
    else:
        print("ðŸŽ¬ No timeline file found, falling back to frames directory")
        total_duration, _ = create_concat_file_from_frames_only(FRAMES_DIR, concat_txt)

    # ------------------ RENDER VIDEO ------------------
    print(f"ðŸŽ¬ Rendering video with total duration: {total_duration}s")
    temp_video = os.path.join(TMP_DIR, "temp_video.mp4")
    
    # Debug: Check if concat file exists and has content
    if os.path.exists(concat_txt):
        with open(concat_txt, "r", encoding="utf-8") as f:
            concat_content = f.read()
        print(f"ðŸŽ¬ Concat file content preview (first 500 chars):")
        print(concat_content[:500])
        print(f"ðŸŽ¬ Concat file has {len(concat_content.splitlines())} lines")
    else:
        print("âŒ Concat file not created!")

    _run(
        f'ffmpeg -y -f concat -safe 0 -i "{concat_txt}" '
        f'-vf "scale=trunc(iw/2)*2:trunc(ih/2)*2" -r {FPS} -pix_fmt yuv420p '
        f'-c:v libx264 -preset veryfast -crf 18 -movflags +faststart "{temp_video}"'
    )

    # Check if temp video was created and get its actual duration
    if os.path.exists(temp_video):
        try:
            result = subprocess.run([
                'ffprobe', '-v', 'error', 
                '-show_entries', 'format=duration', 
                '-of', 'default=noprint_wrappers=1:nokey=1',
                temp_video
            ], capture_output=True, text=True, check=True)
            actual_temp_duration = float(result.stdout.strip())
            print(f"ðŸŽ¬ Temp video duration: {actual_temp_duration}s")
        except Exception as e:
            print(f"âš ï¸ Could not get temp video duration: {e}")
    else:
        print("âŒ Temp video not created!")

    final_audio = os.path.join(TMP_DIR, "final_audio.aac")

    # ------------------ TYPING SOUND EFFECTS ------------------
    print("ðŸŽ¹ ===== TYPING SOUND GENERATION =====")

    # Initialize sound effects list
    delayed_files = []
    print(f"ðŸŽ¹ Initial delayed_files count: {len(delayed_files)}")

    # ========== SINGLE TYPING AUDIO GENERATION METHOD ==========
    if typing_bar_audio and timeline:
        print("ðŸŽ¹ Generating typing audio sessions...")
        
        # Use the FIXED function to create trimmed typing sessions
        typing_map = build_typing_audio_sessions(
            timeline=timeline,
            typing_sound_master_path=ensure_local(typing_bar_audio),
            tmp_dir=os.path.join(TMP_DIR, "typing_sessions")
        )
        
        # Add each typing session audio to delayed_files with proper timing
        for session_id, session_info in typing_map.items():
            start_sec = session_info["start_time"]
            
            # Create delayed audio file
            millis = int(math.floor(start_sec * 1000))
            delayed_typing = os.path.join(TMP_DIR, f"typing_session_{session_id}.aac")
            
            try:
                cmd = [
                    'ffmpeg', '-y', '-hide_banner', '-loglevel', 'error',
                    '-i', session_info["file"],
                    '-af', f'adelay={millis}|{millis}',
                    '-c:a', 'aac', '-b:a', '192k',
                    delayed_typing
                ]
                subprocess.run(cmd, check=True, capture_output=True)
                
                delayed_files.append(delayed_typing)
                print(f"ðŸŽ¹ âœ… Added typing session {session_id}: {session_info['duration']:.3f}s at {start_sec:.3f}s")
                
            except subprocess.CalledProcessError as e:
                print(f"âŒ Failed to delay typing audio for session {session_id}: {e}")
            except Exception as e:
                print(f"âŒ Error delaying typing audio for session {session_id}: {e}")
        
        print(f"ðŸŽ¹ Final: {len(typing_map)} typing audio sessions added")

    else:
        print("ðŸŽ¹ No typing_bar_audio provided or no timeline - skipping typing sounds")
        if typing_audio:
            print(f"ðŸŽ¹ Fallback typing_audio available: {typing_audio}")

    # ------------------ BACKGROUND AUDIO ------------------
    print(f"ðŸŽµ BG Segments parameter received: {bg_segments}")

    # Use passed segments if available, otherwise load from file
    if use_segments:
        if bg_segments is not None and len(bg_segments) > 0:
            print(f"ðŸŽµ Using BG segments passed as parameter: {len(bg_segments)} segments")
            # keep bg_segments as passed
        elif os.path.exists(BG_TIMELINE_FILE):
            with open(BG_TIMELINE_FILE, "r", encoding="utf-8") as f:
                try:
                    bg_segments = json.load(f)
                except Exception as e:
                    print(f"âš ï¸ Failed to parse {BG_TIMELINE_FILE}: {e}")
                    bg_segments = []
            print(f"ðŸŽµ Loaded BG segments from file: {len(bg_segments)} segments")
        else:
            print("ðŸŽµ No BG segments found - parameter was None and file doesn't exist")
            bg_segments = []
    else:
        bg_segments = []
        print("ðŸŽµ Not using segments (use_segments=False)")

    # Track song positions for "continue" mode
    song_positions: Dict[str, float] = {}  # {audio_file: last_played_position}

    # Debug: print all segments being processed with their playback modes
    print("ðŸŽµ ===== SEGMENTS TO PROCESS =====")
    for i, seg in enumerate(bg_segments):
        audio_file = seg.get("audio", "")
        playback_mode = seg.get("playback_mode", "start_fresh")
        custom_start = seg.get("custom_start", 0.0)
        exists = "EXISTS" if os.path.exists(ensure_local(audio_file)) else "MISSING"
        duration = seg["end"] - seg["start"]
        mode_display = {
            "start_fresh": "ðŸ†• Start Fresh",
            "continue": "ðŸ”„ Continue",
            "custom_start": f"â±ï¸ Custom Start ({custom_start}s)"
        }
        print(f"ðŸŽµ   Segment {i}: {seg['start']}s - {seg['end']}s ({duration}s) -> {audio_file} [{exists}] - {mode_display.get(playback_mode, 'ðŸ†• Start Fresh')}")
    print("ðŸŽµ ===============================")

    # Check if user has defined any segments (opted in)
    has_user_defined_segments = len(bg_segments) > 0

    if has_user_defined_segments:
        print("ðŸŽµ User has defined BG segments - using segment-based audio (with silence for gaps)")
        
        # Fill any gaps with silence
        filled_segments = []
        current_time = 0.0
        
        # Sort segments by start time
        bg_segments.sort(key=lambda x: x["start"])
        
        # Add silence before first segment if needed
        if bg_segments and bg_segments[0]["start"] > 0:
            filled_segments.append({
                "start": 0.0,
                "end": bg_segments[0]["start"],
                "audio": "",
                "playback_mode": "start_fresh",
                "custom_start": 0.0
            })
        
        # Process all segments
        for i, seg in enumerate(bg_segments):
            filled_segments.append(seg)
            
            # Check for gap before next segment
            if i < len(bg_segments) - 1:
                next_start = bg_segments[i + 1]["start"]
                if seg["end"] < next_start:
                    filled_segments.append({
                        "start": seg["end"],
                        "end": next_start,
                        "audio": "",
                        "playback_mode": "start_fresh",
                        "custom_start": 0.0
                    })
        
        # Add silence after last segment if needed
        if bg_segments and bg_segments[-1]["end"] < total_duration:
            filled_segments.append({
                "start": bg_segments[-1]["end"],
                "end": total_duration,
                "audio": "",
                "playback_mode": "start_fresh",
                "custom_start": 0.0
            })
        
        # If no segments but file exists (empty array), fill entire duration with silence
        if not bg_segments:
            filled_segments.append({
                "start": 0.0,
                "end": total_duration,
                "audio": "",
                "playback_mode": "start_fresh",
                "custom_start": 0.0
            })
        
        # Process all filled segments
        for seg_idx, seg in enumerate(filled_segments):
            audio_path = seg.get("audio", "")
            playback_mode = seg.get("playback_mode", "start_fresh")
            custom_start = seg.get("custom_start", 0.0)
            seg_dur = seg["end"] - seg["start"]
            
            if seg_dur <= 0:
                continue
                
            # Check if this is a silence segment (empty audio path)
            if not audio_path or not os.path.exists(ensure_local(audio_path)):
                # Create silent clip for silence segments
                silent_clip = os.path.join(TMP_DIR, f"silent_seg_{seg_idx}.aac")
                _run(f'ffmpeg -y -f lavfi -i anullsrc=channel_layout=stereo:sample_rate=44100 -t {seg_dur:.3f} -c:a aac -b:a 192k "{silent_clip}"')
                millis = int(math.floor(seg["start"] * 1000))
                delayed_silent = os.path.join(TMP_DIR, f"delayed_silent_{seg_idx}.aac")
                _run(f'ffmpeg -y -i "{silent_clip}" -af "adelay={millis}|{millis}" "{delayed_silent}"')
                delayed_bg_files.append(delayed_silent)
                print(f"ðŸ”‡ Silence segment: {seg['start']:.1f}-{seg['end']:.1f}s")
            else:
                # Create audio clip for segments with audio
                audio_path = ensure_local(audio_path)
                if os.path.exists(audio_path):
                    # Determine start offset based on playback mode
                    start_offset = 0.0
                    
                    if playback_mode == "continue":
                        # Continue from where this song last left off
                        last_position = song_positions.get(audio_path, 0.0)
                        start_offset = last_position
                        print(f"ðŸ”„ Continuing {os.path.basename(audio_path)} from {last_position:.2f}s")
                        
                    elif playback_mode == "custom_start":
                        # Use custom start time
                        start_offset = custom_start
                        print(f"â±ï¸ Starting {os.path.basename(audio_path)} from custom time: {custom_start:.2f}s")
                    
                    # else: "start_fresh" uses start_offset = 0.0
                    
                    # Create the audio clip with the appropriate start offset
                    bg_clip = os.path.join(TMP_DIR, f"bg_seg_{seg_idx}.aac")
                    
                    # Use ffmpeg to extract portion starting from offset
                    _run(f'ffmpeg -y -ss {start_offset:.3f} -i "{audio_path}" -t {seg_dur:.3f} -c:a aac -b:a 192k "{bg_clip}"')
                    
                    # Update song position for "continue" mode
                    if playback_mode == "continue":
                        new_position = start_offset + seg_dur
                        song_positions[audio_path] = new_position
                        print(f"ðŸ“ Updated {os.path.basename(audio_path)} position: {new_position:.2f}s")
                    
                    millis = int(math.floor(seg["start"] * 1000))
                    delayed_bg = os.path.join(TMP_DIR, f"delayed_bg_{seg_idx}.aac")
                    _run(f'ffmpeg -y -i "{bg_clip}" -af "adelay={millis}|{millis}" "{delayed_bg}"')
                    delayed_bg_files.append(delayed_bg)
                    
                    mode_display = {
                        "start_fresh": "ðŸ†• Start Fresh",
                        "continue": "ðŸ”„ Continue",
                        "custom_start": f"â±ï¸ Custom Start ({custom_start}s)"
                    }
                    
                    print(f"ðŸŽµ Audio segment: {seg['start']:.1f}-{seg['end']:.1f}s - {os.path.basename(audio_path)} - {mode_display.get(playback_mode, 'ðŸ†• Start Fresh')}")
                else:
                    print(f"âš ï¸ Audio file not found: {audio_path}, using silence")
                    # Fallback to silence
                    silent_clip = os.path.join(TMP_DIR, f"silent_seg_{seg_idx}.aac")
                    _run(f'ffmpeg -y -f lavfi -i anullsrc=channel_layout=stereo:sample_rate=44100 -t {seg_dur:.3f} -c:a aac -b:a 192k "{silent_clip}"')
                    millis = int(math.floor(seg["start"] * 1000))
                    delayed_silent = os.path.join(TMP_DIR, f"delayed_silent_{seg_idx}.aac")
                    _run(f'ffmpeg -y -i "{silent_clip}" -af "adelay={millis}|{millis}" "{delayed_silent}"')
                    delayed_bg_files.append(delayed_silent)
        
        print(f"ðŸŽµ Processed {len(filled_segments)} BG segments ({len(delayed_bg_files)} audio files)")
        print(f"ðŸŽµ Final song positions: {song_positions}")

    else:
        # No segments defined - use default background for entire video
        print("ðŸŽµ No BG segments defined - using default background audio")
        if bg_audio and os.path.exists(ensure_local(bg_audio)):
            bg_loop = os.path.join(TMP_DIR, "bg_loop.aac")
            _run(f'ffmpeg -y -stream_loop -1 -i "{ensure_local(bg_audio)}" -t {total_duration:.3f} -c:a aac -b:a 192k "{bg_loop}"')
            delayed_bg_files = [bg_loop]
            print(f"ðŸ”Š Using default background: {os.path.basename(bg_audio)}")
        else:
            print("âš ï¸ No valid background audio provided, rendering without background music")

    # ------------------ MESSAGE SOUND EFFECTS ------------------
    print("ðŸŽµ Processing message sounds...")
    current_time = 0.0
    sound_idx = len(delayed_files)  # Start from current count

    for i, entry in enumerate(timeline):
        dur = float(entry.get("duration", 1.0))
    
        # Skip typing entries (we already handled them with sessions)
        if entry.get("typing") or entry.get("typing_bar"):
            current_time += dur
            continue
    
        # Message sounds (send/recv) for regular messages
        has_content = entry.get("text") or entry.get("is_meme")
        if has_content:
            sound_delay = current_time + 0.5
            audio_file = ensure_local(send_audio if entry.get("is_sender") else recv_audio)
        
            if audio_file and os.path.exists(audio_file):
                out_del = os.path.join(TMP_DIR, f"msg_{sound_idx}.wav")
                _run(f'ffmpeg -y -i "{audio_file}" -af "adelay={int(sound_delay*1000)}|{int(sound_delay*1000)}" "{out_del}"')
                delayed_files.append(out_del)
                sound_idx += 1
                print(f"ðŸŽµ   âœ… Message sound at {sound_delay:.2f}s")

        current_time += dur

    print(f"ðŸŽµ ===== SOUND EFFECTS DEBUG END =====")
    print(f"ðŸŽµ Total sound effects generated: {len(delayed_files)}")
    for i, sound_file in enumerate(delayed_files):
        exists = "âœ…" if os.path.exists(sound_file) else "âŒ"
        print(f"ðŸŽµ   {i}: {exists} {os.path.basename(sound_file)}")

    # ------------------ FINAL AUDIO MIX ------------------
    print(f"ðŸŽµ Mixing {len(delayed_bg_files)} background files + {len(delayed_files)} sound effects")

    # Use a more efficient approach for many inputs
    all_audio_files = delayed_bg_files + delayed_files

    if len(all_audio_files) == 0:
        # No audio at all
        final_video = OUTPUT_VIDEO
        _run(f'ffmpeg -y -i "{temp_video}" -c:v copy -an "{final_video}"')
    elif len(all_audio_files) <= 30:
        # For reasonable number of inputs, use normal amix
        inputs = " ".join(f'-i "{p}"' for p in all_audio_files)
        num_inputs = len(all_audio_files)
        labels = "".join(f'[{i}:a]' for i in range(num_inputs))
        _run(
            f'ffmpeg -y {inputs} -filter_complex "{labels}amix=inputs={num_inputs}:normalize=0:dropout_transition=0" '
            f'-c:a aac -b:a 192k "{final_audio}"'
        )
    else:
        # For many inputs, mix in stages to avoid command line limits
        print(f"ðŸŽµ Many audio inputs ({len(all_audio_files)}), mixing in stages...")
    
        # Mix background files first
        if delayed_bg_files:
            bg_inputs = " ".join(f'-i "{p}"' for p in delayed_bg_files)
            bg_mixed = os.path.join(TMP_DIR, "bg_mixed.aac")
            bg_labels = "".join(f'[{i}:a]' for i in range(len(delayed_bg_files)))
            _run(
                f'ffmpeg -y {bg_inputs} -filter_complex "{bg_labels}amix=inputs={len(delayed_bg_files)}:normalize=0" '
                f'-c:a aac -b:a 192k "{bg_mixed}"'
            )
            # Now mix background with sound effects
            if delayed_files:
                se_inputs = " ".join(f'-i "{p}"' for p in delayed_files)
                se_labels = "".join(f'[{i}:a]' for i in range(len(delayed_files)))
                _run(
                    f'ffmpeg -y -i "{bg_mixed}" {se_inputs} '
                    f'-filter_complex "[0:a]{se_labels}amix=inputs={len(delayed_files)+1}:normalize=0:dropout_transition=0" '
                    f'-c:a aac -b:a 192k "{final_audio}"'
                )
            else:
                final_audio = bg_mixed
        else:
            # Only sound effects, no background
            se_inputs = " ".join(f'-i "{p}"' for p in delayed_files)
            se_labels = "".join(f'[{i}:a]' for i in range(len(delayed_files)))
            _run(
                f'ffmpeg -y {se_inputs} -filter_complex "{se_labels}amix=inputs={len(delayed_files)}:normalize=0:dropout_transition=0" '
                f'-c:a aac -b:a 192k "{final_audio}"'
            )

    final_video = OUTPUT_VIDEO
    _run(
        f'ffmpeg -y -i "{temp_video}" -i "{final_audio}" -c:v copy -c:a aac -shortest -movflags +faststart "{final_video}"'
    )
    
    # Final debug: check the actual duration of the output video
    if os.path.exists(final_video):
        try:
            result = subprocess.run([
                'ffprobe', '-v', 'error', 
                '-show_entries', 'format=duration', 
                '-of', 'default=noprint_wrappers=1:nokey=1',
                final_video
            ], capture_output=True, text=True, check=True)
            actual_final_duration = float(result.stdout.strip())
            print(f"ðŸŽ¬ Final video duration: {actual_final_duration}s")
            print(f"ðŸŽ¬ Expected duration: {total_duration}s")
        except Exception as e:
            print(f"âš ï¸ Could not get final video duration: {e}")
    
    print(f"ðŸŽ¬ Final video saved to: {final_video}")
    return final_video